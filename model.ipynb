{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Strojove-uceni/23206-final-sos/blob/main/model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Activation layer"
      ],
      "metadata": {
        "id": "3fNYaxNgK9nE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def activation_layer(activation: str=\"relu\", alpha: float=0.1, inplace: bool=True):\n",
        "    \"\"\" Activation layer wrapper for LeakyReLU and ReLU activation functions\n",
        "\n",
        "    Args:\n",
        "        activation: str, activation function name (default: 'relu')\n",
        "        alpha: float (LeakyReLU activation function parameter)\n",
        "\n",
        "    Returns:\n",
        "        torch.Tensor: activation layer\n",
        "    \"\"\"\n",
        "    if activation == \"relu\":\n",
        "        return nn.ReLU(inplace=inplace)\n",
        "\n",
        "    elif activation == \"leaky_relu\":\n",
        "        return nn.LeakyReLU(negative_slope=alpha, inplace=inplace)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P9aTd7EVJrk6",
        "outputId": "62cbca2b-bcb9-48de-e68f-86ae51f585aa"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting lightning\n",
            "  Downloading lightning-2.1.2-py3-none-any.whl (2.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m10.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: PyYAML<8.0,>=5.4 in /usr/local/lib/python3.10/dist-packages (from lightning) (6.0.1)\n",
            "Requirement already satisfied: fsspec[http]<2025.0,>2021.06.0 in /usr/local/lib/python3.10/dist-packages (from lightning) (2023.6.0)\n",
            "Collecting lightning-utilities<2.0,>=0.8.0 (from lightning)\n",
            "  Downloading lightning_utilities-0.10.0-py3-none-any.whl (24 kB)\n",
            "Requirement already satisfied: numpy<3.0,>=1.17.2 in /usr/local/lib/python3.10/dist-packages (from lightning) (1.23.5)\n",
            "Requirement already satisfied: packaging<25.0,>=20.0 in /usr/local/lib/python3.10/dist-packages (from lightning) (23.2)\n",
            "Requirement already satisfied: torch<4.0,>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from lightning) (2.1.0+cu121)\n",
            "Collecting torchmetrics<3.0,>=0.7.0 (from lightning)\n",
            "  Downloading torchmetrics-1.2.1-py3-none-any.whl (806 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m806.1/806.1 kB\u001b[0m \u001b[31m55.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm<6.0,>=4.57.0 in /usr/local/lib/python3.10/dist-packages (from lightning) (4.66.1)\n",
            "Requirement already satisfied: typing-extensions<6.0,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from lightning) (4.5.0)\n",
            "Collecting pytorch-lightning (from lightning)\n",
            "  Downloading pytorch_lightning-2.1.2-py3-none-any.whl (776 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m776.9/776.9 kB\u001b[0m \u001b[31m62.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from fsspec[http]<2025.0,>2021.06.0->lightning) (2.31.0)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]<2025.0,>2021.06.0->lightning) (3.9.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from lightning-utilities<2.0,>=0.8.0->lightning) (67.7.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch<4.0,>=1.12.0->lightning) (3.13.1)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch<4.0,>=1.12.0->lightning) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch<4.0,>=1.12.0->lightning) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch<4.0,>=1.12.0->lightning) (3.1.2)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch<4.0,>=1.12.0->lightning) (2.1.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2025.0,>2021.06.0->lightning) (23.1.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2025.0,>2021.06.0->lightning) (6.0.4)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2025.0,>2021.06.0->lightning) (1.9.4)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2025.0,>2021.06.0->lightning) (1.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2025.0,>2021.06.0->lightning) (1.3.1)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2025.0,>2021.06.0->lightning) (4.0.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch<4.0,>=1.12.0->lightning) (2.1.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->fsspec[http]<2025.0,>2021.06.0->lightning) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->fsspec[http]<2025.0,>2021.06.0->lightning) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->fsspec[http]<2025.0,>2021.06.0->lightning) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->fsspec[http]<2025.0,>2021.06.0->lightning) (2023.11.17)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch<4.0,>=1.12.0->lightning) (1.3.0)\n",
            "Installing collected packages: lightning-utilities, torchmetrics, pytorch-lightning, lightning\n",
            "Successfully installed lightning-2.1.2 lightning-utilities-0.10.0 pytorch-lightning-2.1.2 torchmetrics-1.2.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Residual block"
      ],
      "metadata": {
        "id": "FW7Nmpt9LAZv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ConvBlock(nn.Module):\n",
        "    \"\"\" Convolutional block with batch normalization\n",
        "    \"\"\"\n",
        "    def __init__(self, in_channels: int, out_channels: int, kernel_size: int, stride: int, padding: int):\n",
        "        super(ConvBlock, self).__init__()\n",
        "        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size=kernel_size, stride=stride, padding=padding)\n",
        "        self.bn = nn.BatchNorm2d(out_channels)\n",
        "\n",
        "    def forward(self, x: torch.Tensor):\n",
        "        return self.bn(self.conv(x))\n",
        "\n",
        "\n",
        "class ResidualBlock(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, skip_conv=True, stride=1, dropout=0.2, activation=\"leaky_relu\"):\n",
        "        super(ResidualBlock, self).__init__()\n",
        "        self.convb1 = ConvBlock(in_channels, out_channels, kernel_size=3, stride=stride, padding=1)\n",
        "        self.act1 = activation_layer(activation)\n",
        "\n",
        "        self.convb2 = ConvBlock(out_channels, out_channels, kernel_size=3, stride=1, padding=1)\n",
        "\n",
        "        self.dropout = nn.Dropout(p=dropout)\n",
        "\n",
        "        self.shortcut = None\n",
        "        if skip_conv:\n",
        "            if stride != 1 or in_channels != out_channels:\n",
        "                self.shortcut = nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=stride)\n",
        "\n",
        "        self.act2 = activation_layer(activation)\n",
        "\n",
        "    def forward(self, x):\n",
        "        skip = x\n",
        "\n",
        "        out = self.act1(self.convb1(x))\n",
        "        out = self.convb2(out)\n",
        "\n",
        "        if self.shortcut is not None:\n",
        "            out += self.shortcut(skip)\n",
        "\n",
        "        out = self.act2(out)\n",
        "        out = self.dropout(out)\n",
        "\n",
        "        return out"
      ],
      "metadata": {
        "id": "eHlPQIMXK4Sk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Model"
      ],
      "metadata": {
        "id": "H0CmxzD_J5c_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CNNmodel(pl.LightningModule):\n",
        "    def __init__(self, pad_val, num_chars: int, activation: str=\"leaky_relu\", dropout: float=0.2):\n",
        "        super(CNNmodel, self).__init__()\n",
        "\n",
        "        self.pad_val = pad_val\n",
        "\n",
        "        self.rb1 = ResidualBlock(3, 16, skip_conv=True, stride=1, activation=activation, dropout=dropout)\n",
        "        self.rb2 = ResidualBlock(16, 16, skip_conv=True, stride=2, activation=activation, dropout=dropout)\n",
        "        self.rb3 = ResidualBlock(16, 16, skip_conv=False, stride=1, activation=activation, dropout=dropout)\n",
        "        self.rb4 = ResidualBlock(16, 32, skip_conv=True, stride=2, activation=activation, dropout=dropout)\n",
        "        self.rb5 = ResidualBlock(32, 32, skip_conv=False, stride=1, activation=activation, dropout=dropout)\n",
        "        self.rb6 = ResidualBlock(32, 64, skip_conv=True, stride=2, activation=activation, dropout=dropout)\n",
        "        self.rb7 = ResidualBlock(64, 64, skip_conv=True, stride=1, activation=activation, dropout=dropout)\n",
        "        self.rb8 = ResidualBlock(64, 64, skip_conv=False, stride=1, activation=activation, dropout=dropout)\n",
        "        self.rb9 = ResidualBlock(64, 64, skip_conv=False, stride=1, activation=activation, dropout=dropout)\n",
        "\n",
        "        self.lstm = nn.LSTM(64, 128, bidirectional=True, num_layers=1, batch_first=True)\n",
        "        self.lstm_dropout = nn.Dropout(p=dropout)\n",
        "\n",
        "        self.output = nn.Linear(256, num_chars + 1)\n",
        "\n",
        "    def forward(self, images: torch.Tensor) -> torch.Tensor:\n",
        "        images_float = images / 255.0\n",
        "        #images_float = images_float.permute(0, 3, 1, 2)\n",
        "\n",
        "        #print(f\"Shape after initial processing: {images_float.shape}\")\n",
        "\n",
        "#------------------------------------------------------------------------------\n",
        "        x = self.rb1(images_float)\n",
        "        #print(f\"After rb1 Shape: {x.shape}\")\n",
        "        x = self.rb2(x)\n",
        "        #print(f\"After rb2 Shape: {x.shape}\")\n",
        "        x = self.rb3(x)\n",
        "        #print(f\"After rb3 Shape: {x.shape}\")\n",
        "        x = self.rb4(x)\n",
        "        #print(f\"After rb4 Shape: {x.shape}\")\n",
        "        x = self.rb5(x)\n",
        "        #print(f\"After rb5 Shape: {x.shape}\")\n",
        "        x = self.rb6(x)\n",
        "        #print(f\"After rb6 Shape: {x.shape}\")\n",
        "        x = self.rb7(x)\n",
        "        #print(f\"After rb7 Shape: {x.shape}\")\n",
        "        x = self.rb8(x)\n",
        "        #print(f\"After rb8 Shape: {x.shape}\")\n",
        "        x = self.rb9(x)\n",
        "        #print(f\"After rb9 Shape: {x.shape}\")\n",
        "\n",
        "        x = x.reshape(x.size(0), -1, x.size(1))\n",
        "        #print(f\"After Reshape Shape: {x.shape}\")\n",
        "\n",
        "        x, _ = self.lstm(x)\n",
        "        x = self.lstm_dropout(x)\n",
        "        #print(f\"After LSTM Shape: {x.shape}\")\n",
        "\n",
        "        x = self.output(x)\n",
        "        x = F.log_softmax(x, 2)\n",
        "        #print('X from soft_max:', x)\n",
        "        #print(f\"Final Output Shape: {x.shape}\")\n",
        "#------------------------------------------------------------------------------\n",
        "\n",
        "        return x\n",
        "\n",
        "    def training_step(self, batch, batch_idx):\n",
        "        images, targets = batch\n",
        "\n",
        "        outputs = self(images)\n",
        "\n",
        "        target_lengths = torch.sum(targets != self.pad_val, dim=1)\n",
        "\n",
        "        targets_unpadded = targets[targets != self.pad_val].view(-1)\n",
        "\n",
        "        outputs = outputs.permute(1, 0, 2)  # (sequence_length, batch_size, num_classes)\n",
        "        outputs_lengths = torch.full(size=(outputs.size(1),), fill_value=outputs.size(0), dtype=torch.long)\n",
        "\n",
        "        #print('Input lenghts tr', len(outputs_lengths))\n",
        "        loss = F.ctc_loss(outputs, targets_unpadded, outputs_lengths, target_lengths, blank = self.pad_val)\n",
        "        self.log('train_loss', loss, on_step=True, on_epoch=True, prog_bar=True, logger=True)\n",
        "        return loss\n",
        "\n",
        "    def validation_step(self, batch, batch_idx):\n",
        "        images, targets = batch\n",
        "\n",
        "        outputs = self(images)\n",
        "\n",
        "        target_lengths = torch.sum(targets != self.pad_val, dim=1)\n",
        "\n",
        "        targets_unpadded = targets[targets != self.pad_val].view(-1)\n",
        "\n",
        "        outputs = outputs.permute(1, 0, 2)  # (sequence_length, batch_size, num_classes)\n",
        "        outputs_lengths = torch.full(size=(outputs.size(1),), fill_value=outputs.size(0), dtype=torch.long)\n",
        "\n",
        "        #print('Input lenghts val', len(outputs_lengths))\n",
        "        loss = F.ctc_loss(outputs, targets_unpadded, outputs_lengths, target_lengths, blank = self.pad_val)\n",
        "        self.log('val_loss', loss, on_step=True, on_epoch=True, prog_bar=True, logger=True)\n",
        "        return loss\n",
        "\n",
        "    def configure_optimizers(self):\n",
        "        optimizer = torch.optim.Adam(self.parameters(), lr=1e-6)\n",
        "        return optimizer"
      ],
      "metadata": {
        "id": "lE8TzLhmJaWl"
      },
      "execution_count": 4,
      "outputs": []
    }
  ]
}