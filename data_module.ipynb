{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Strojove-uceni/23206-final-sos/blob/main/data_module.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install lightning"
      ],
      "metadata": {
        "id": "P9aTd7EVJrk6",
        "outputId": "62cbca2b-bcb9-48de-e68f-86ae51f585aa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting lightning\n",
            "  Downloading lightning-2.1.2-py3-none-any.whl (2.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m10.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: PyYAML<8.0,>=5.4 in /usr/local/lib/python3.10/dist-packages (from lightning) (6.0.1)\n",
            "Requirement already satisfied: fsspec[http]<2025.0,>2021.06.0 in /usr/local/lib/python3.10/dist-packages (from lightning) (2023.6.0)\n",
            "Collecting lightning-utilities<2.0,>=0.8.0 (from lightning)\n",
            "  Downloading lightning_utilities-0.10.0-py3-none-any.whl (24 kB)\n",
            "Requirement already satisfied: numpy<3.0,>=1.17.2 in /usr/local/lib/python3.10/dist-packages (from lightning) (1.23.5)\n",
            "Requirement already satisfied: packaging<25.0,>=20.0 in /usr/local/lib/python3.10/dist-packages (from lightning) (23.2)\n",
            "Requirement already satisfied: torch<4.0,>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from lightning) (2.1.0+cu121)\n",
            "Collecting torchmetrics<3.0,>=0.7.0 (from lightning)\n",
            "  Downloading torchmetrics-1.2.1-py3-none-any.whl (806 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m806.1/806.1 kB\u001b[0m \u001b[31m55.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm<6.0,>=4.57.0 in /usr/local/lib/python3.10/dist-packages (from lightning) (4.66.1)\n",
            "Requirement already satisfied: typing-extensions<6.0,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from lightning) (4.5.0)\n",
            "Collecting pytorch-lightning (from lightning)\n",
            "  Downloading pytorch_lightning-2.1.2-py3-none-any.whl (776 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m776.9/776.9 kB\u001b[0m \u001b[31m62.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from fsspec[http]<2025.0,>2021.06.0->lightning) (2.31.0)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]<2025.0,>2021.06.0->lightning) (3.9.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from lightning-utilities<2.0,>=0.8.0->lightning) (67.7.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch<4.0,>=1.12.0->lightning) (3.13.1)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch<4.0,>=1.12.0->lightning) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch<4.0,>=1.12.0->lightning) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch<4.0,>=1.12.0->lightning) (3.1.2)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch<4.0,>=1.12.0->lightning) (2.1.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2025.0,>2021.06.0->lightning) (23.1.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2025.0,>2021.06.0->lightning) (6.0.4)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2025.0,>2021.06.0->lightning) (1.9.4)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2025.0,>2021.06.0->lightning) (1.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2025.0,>2021.06.0->lightning) (1.3.1)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2025.0,>2021.06.0->lightning) (4.0.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch<4.0,>=1.12.0->lightning) (2.1.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->fsspec[http]<2025.0,>2021.06.0->lightning) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->fsspec[http]<2025.0,>2021.06.0->lightning) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->fsspec[http]<2025.0,>2021.06.0->lightning) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->fsspec[http]<2025.0,>2021.06.0->lightning) (2023.11.17)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch<4.0,>=1.12.0->lightning) (1.3.0)\n",
            "Installing collected packages: lightning-utilities, torchmetrics, pytorch-lightning, lightning\n",
            "Successfully installed lightning-2.1.2 lightning-utilities-0.10.0 pytorch-lightning-2.1.2 torchmetrics-1.2.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "import lightning.pytorch as pl\n",
        "\n",
        "from PIL import Image\n",
        "\n",
        "\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from torchvision import transforms\n",
        "from torchvision.transforms import ToTensor, Resize, ColorJitter, RandomRotation, \\\n",
        "    RandomAdjustSharpness, RandomAutocontrast, AutoAugment"
      ],
      "metadata": {
        "id": "0LzPunpwJfQB"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Dataset"
      ],
      "metadata": {
        "id": "H0CmxzD_J5c_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class IAMWordsDataset(Dataset):\n",
        "    def __init__(self, dataset, vocab, max_len, split=\"train\", train_val_split=0.9):\n",
        "        self.dataset = dataset\n",
        "        self.vocab = vocab\n",
        "        self.max_len = max_len\n",
        "        self.split = split\n",
        "        self.train_val_split = train_val_split\n",
        "\n",
        "        # maybe it should be assigned own vocab and max_len to train/val dataset, but idk\n",
        "        if self.split == \"train\":\n",
        "            self.dataset = self.dataset[:int(len(self.dataset) * self.train_val_split)]\n",
        "        elif self.split == \"val\":\n",
        "            self.dataset = self.dataset[int(len(self.dataset) * self.train_val_split):]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.dataset)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        image_path, label = self.dataset[idx]\n",
        "        image = Image.open(image_path).convert(\"RGB\")\n",
        "#------------------------------------------------------------------------------\n",
        "        #print(\"Hello World\")\n",
        "        #torch.cuda.synchronize(print(f\"Loaded image size: {image.size}\"))\n",
        "#------------------------------------------------------------------------------\n",
        "        return image, label"
      ],
      "metadata": {
        "id": "lE8TzLhmJaWl"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#DataModule"
      ],
      "metadata": {
        "id": "pmpYWdZ0J21V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class TextRecognitionDataModule(pl.LightningDataModule):\n",
        "    def __init__(self, dataset, vocab, max_len, batch_size=64):\n",
        "        super().__init__()\n",
        "        self.batch_size = batch_size\n",
        "        self.dataset = dataset\n",
        "        self.vocab = vocab\n",
        "        self.max_len = max_len\n",
        "\n",
        "    def setup(self, stage=None):\n",
        "        self.train_dataloader()\n",
        "        self.val_dataloader()\n",
        "\n",
        "\n",
        "    def train_dataloader(self):\n",
        "        self.train_dataset = IAMWordsDataset(self.dataset, self.vocab, self.max_len, split=\"train\")\n",
        "        print(\"Train Dataloader called\")\n",
        "        return DataLoader(self.train_dataset, batch_size=self.batch_size, shuffle=True,\n",
        "                          collate_fn=self.train_collate, num_workers=0)\n",
        "\n",
        "    def val_dataloader(self):\n",
        "        self.val_dataset = IAMWordsDataset(self.dataset, self.vocab, self.max_len, split=\"val\")\n",
        "        print(\"Validation Dataloader called\")\n",
        "        return DataLoader(self.val_dataset, batch_size=self.batch_size,\n",
        "                          collate_fn=self.val_collate, num_workers=0)\n",
        "\n",
        "    # ensure the same size of images and convert it to tensor\n",
        "    def train_collate(self, batch):\n",
        "        images, labels = zip(*batch)\n",
        "        #print(labels)\n",
        "\n",
        "        #--------------------------------------------------------------------\n",
        "        # Print the shape of images before processing\n",
        "        #for img in images:\n",
        "            #print(f\"Original image size: {img.size}\")\n",
        "        #--------------------------------------------------------------------\n",
        "\n",
        "        # Resize all images to a consistent size (e.g., 128x32)\n",
        "        resize = transforms.Compose([\n",
        "            Resize((128, 32))])\n",
        "\n",
        "        images = [resize(image) for image in images]\n",
        "\n",
        "        transform = transforms.Compose([\n",
        "            ColorJitter(brightness=0.5),\n",
        "            RandomRotation(degrees=30),\n",
        "            #RandomAffine(degrees=0, shear=10),\n",
        "            #RandomAffine(degrees=0, translate=(0.1, 0.1)),\n",
        "            #RandomAffine(degrees=0, scale=(0.8, 1.2)),\n",
        "            RandomAdjustSharpness(sharpness_factor=2),\n",
        "            RandomAutocontrast(),\n",
        "            #RandomErasing(),\n",
        "            AutoAugment(),\n",
        "            ToTensor()])\n",
        "\n",
        "        images = [transform(image) for image in images]\n",
        "\n",
        "        # Convert labels to integer tensors\n",
        "\n",
        "        converted_labels = []\n",
        "        for label in labels:\n",
        "            converted_label = [self.vocab.index(l) for l in label if l in self.vocab]\n",
        "            converted_labels.append(torch.tensor(converted_label, dtype=torch.long))\n",
        "\n",
        "        #print(converted_labels)\n",
        "\n",
        "        # Pad the labels to the same length for batch processing\n",
        "        padded_labels = torch.nn.utils.rnn.pad_sequence(converted_labels,\n",
        "                                                        batch_first=True,\n",
        "                                                        padding_value=0)\n",
        "        return torch.stack(images), padded_labels\n",
        "\n",
        "    def val_collate(self, batch):\n",
        "        images, labels = zip(*batch)\n",
        "        #print(labels)\n",
        "\n",
        "        #--------------------------------------------------------------------\n",
        "        # Print the shape of images before processing\n",
        "        #for img in images:\n",
        "            #print(f\"Original image size: {img.size}\")\n",
        "        #--------------------------------------------------------------------\n",
        "\n",
        "        # Resize all images to a consistent size (e.g., 128x32)\n",
        "        transform = transforms.Compose([\n",
        "            Resize((128, 32)),\n",
        "            ToTensor()])\n",
        "\n",
        "        images = [transform(image) for image in images]\n",
        "\n",
        "        # Convert labels to integer tensors\n",
        "\n",
        "        converted_labels = []\n",
        "        for label in labels:\n",
        "            converted_label = [self.vocab.index(l) for l in label if l in self.vocab]\n",
        "            converted_labels.append(torch.tensor(converted_label, dtype=torch.long))\n",
        "\n",
        "        #print(converted_labels)\n",
        "\n",
        "        # Pad the labels to the same length for batch processing\n",
        "        padded_labels = torch.nn.utils.rnn.pad_sequence(converted_labels,\n",
        "                                                        batch_first=True,\n",
        "                                                        padding_value=0)\n",
        "        return torch.stack(images), padded_labels"
      ],
      "metadata": {
        "id": "fGjBfxAeJ0Sv"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}